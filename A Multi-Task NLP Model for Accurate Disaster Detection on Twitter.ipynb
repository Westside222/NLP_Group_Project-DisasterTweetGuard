{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb04ff20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Team Members: JIANWEI Luo, WEIFENG Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86b39d",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026fa482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification,AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b1444e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266e3daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "def load_datasets(Task_1_folder, Task_2_folder):\n",
    "    # read csv file\n",
    "    Task_1_train_file_path = os.path.join(Task_1_folder, \"data/train.csv\")\n",
    "    Task_1_test_file_path = os.path.join(Task_1_folder, \"data/test.csv\")\n",
    "    Task_2_data_file_path = os.path.join(Task_2_folder, \"data/Tweets.csv\")\n",
    "    Task_1_train = pd.read_csv(Task_1_train_file_path)\n",
    "    Task_1_test = pd.read_csv(Task_1_test_file_path)\n",
    "    Task_2_data = pd.read_csv(Task_2_data_file_path)\n",
    "    # nan text processing and label mapping\n",
    "    Task_2_data['text'] = Task_2_data['text'].replace(np.nan,\" \")\n",
    "    label2index = {\n",
    "        \"negative\":0,\n",
    "        \"neutral\":1,\n",
    "        \"positive\":2\n",
    "    }\n",
    "    Task_2_data['sentiment'] = Task_2_data['sentiment'].map(label2index)\n",
    "    return Task_1_train, Task_1_test, Task_2_data\n",
    "Task_1_folder = \"./Task_1\"\n",
    "Task_2_folder = \"./Task_2\"\n",
    "Task_1_train, Task_1_test, Task_2_data = load_datasets(Task_1_folder, Task_2_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed911ea",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "159cdccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine the text from task1 and task2\n",
    "new_corpus = pd.DataFrame()\n",
    "new_corpus['text'] = pd.concat([Task_1_train['text'], Task_1_test['text'], Task_2_data['text']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57d2b144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the words and prepare a pipeline for tensor representation of text.\n",
    "class Text_Dataset(Dataset):\n",
    "    def __init__(self, dataset, pretrain_path):\n",
    "        super(Text_Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrain_path)\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset['text'].iloc[index]\n",
    "        encode_dict = self.tokenizer.encode_plus(text=text, max_length=512,padding='max_length', truncation=True)\n",
    "        return encode_dict['input_ids'], encode_dict['attention_mask']\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "919ab3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "text_dataset = Text_Dataset(new_corpus, pretrain_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9783c8ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the training data\n",
    "Task_1_train_X, Task_1_train_Y = Task_1_train['text'], Task_1_train['target']\n",
    "Task_1_train_X, Task_1_val_X, Task_1_train_Y, Task_1_val_Y = train_test_split( Task_1_train_X, Task_1_train_Y, test_size=0.2, random_state=42)\n",
    "Task_2_train_X, Task_2_train_Y = Task_2_data['text'], Task_2_data['sentiment']\n",
    "Task_2_train_X, Task_2_val_X, Task_2_train_Y, Task_2_val_Y = train_test_split( Task_2_train_X, Task_2_train_Y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560fd6b-ee49-47d6-a060-02ef9bc7f9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d41115f",
   "metadata": {},
   "source": [
    "## Model building: we choose roberta as our pretrained model and we will explain it in our report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a2c7575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./roberta/roberta_base/roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./roberta/roberta_base/roberta_base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "Task_1_Bert = AutoModelForSequenceClassification.from_pretrained(pretrain_path,num_labels=2)\n",
    "Task_2_Bert = AutoModelForSequenceClassification.from_pretrained(pretrain_path,num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95a7146",
   "metadata": {},
   "source": [
    "## III. Neural Multi-Task Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d0fe8e",
   "metadata": {},
   "source": [
    "1. Train the model $M$ only for $T_1$ on $D_1^{train}$. Call the trained model $M_{D_1}$. Evaluate $M_{D_1}$ on $D_1^{val}$ and report its performance metric (F1 score) for the first task, ${Perf}_{T_1}(M_{D_1}|D_1^{val})$. Also print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ad5ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model M1/M2 only for T1/T2 on D1_train/D2_train.T1 calculate F1 on D1_val, T2 calculate ACC on D1_val.\n",
    "# bulid dataset\n",
    "class Task_Dataset(Dataset):\n",
    "    def __init__(self, dataset_X, dataset_Y, pretrain_path):\n",
    "        super(Task_Dataset, self).__init__()\n",
    "        self.dataset_X = dataset_X\n",
    "        self.dataset_Y = dataset_Y\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrain_path)\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset_X.iloc[index]\n",
    "        target = self.dataset_Y.iloc[index]\n",
    "        encode_dict = self.tokenizer.encode_plus(text=text, max_length=512,padding='max_length', truncation=True)\n",
    "        return encode_dict['input_ids'], encode_dict['attention_mask'], target\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_X)\n",
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "Task_1_train_dataset = Task_Dataset(Task_1_train_X, Task_1_train_Y, pretrain_path)\n",
    "Task_1_val_dataset = Task_Dataset(Task_1_val_X, Task_1_val_Y, pretrain_path)\n",
    "Task_2_train_dataset = Task_Dataset(Task_2_train_X, Task_2_train_Y, pretrain_path)\n",
    "Task_2_val_dataset = Task_Dataset(Task_2_val_X, Task_2_val_Y, pretrain_path)\n",
    "# bulid dataloader\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    def padding_collate(batch):\n",
    "        (input_ids, attention_masks, labels) = zip(*batch)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "        attention_masks = torch.tensor(attention_masks).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        return {\"input_ids\":input_ids, \"attention_mask\":attention_masks, \"labels\":labels}\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=padding_collate)\n",
    "Task_1_train_Dataloader = get_dataloader(Task_1_train_dataset, 8, False)\n",
    "Task_1_val_Dataloader = get_dataloader(Task_1_val_dataset, 8, False)\n",
    "Task_2_train_Dataloader = get_dataloader(Task_2_train_dataset, 8, False)\n",
    "Task_2_val_Dataloader = get_dataloader(Task_2_val_dataset, 8, False)\n",
    "# train_val\n",
    "def train(bert_model, train_Dataloader, val_Dataloader, task_type, metric_type, epoch, lr, weight_decay):\n",
    "    bert_model.to(device)\n",
    "    optimizer = AdamW(bert_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    bert_model.train()\n",
    "    for e in range(epoch):\n",
    "        loss_all = 0\n",
    "        for index, batch in enumerate(train_Dataloader):\n",
    "            output = bert_model(**batch)\n",
    "            loss = output.loss\n",
    "            loss_all += loss.data\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"Epoch:{}/{}, Loss:{}\".format(e+1,epoch,loss_all/len(train_Dataloader)))\n",
    "    bert_model.eval()\n",
    "    prediction_label_list, true_label_list  = [], []\n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(val_Dataloader):\n",
    "            output = bert_model(**batch)\n",
    "            prediction_label_list.extend(output.logits.argmax(1).cpu().numpy().tolist())\n",
    "            true_label_list.extend(batch['labels'].cpu().numpy().tolist())\n",
    "    save_path = os.path.join(task_type, \"trained_model\")\n",
    "    bert_model.save_pretrained(save_path)\n",
    "    print(\"Save Model in\", save_path)\n",
    "    if metric_type==\"F1\":\n",
    "        print(task_type, \"F1 on val Dataset:{:.2f}\".format(f1_score(true_label_list, prediction_label_list)))\n",
    "    else:\n",
    "        print(task_type, \"Acc on val Dataset:{:.2f}\".format(accuracy_score(true_label_list, prediction_label_list)))\n",
    "    print(task_type, \"confusion_matrix:\")\n",
    "    print(confusion_matrix(true_label_list, prediction_label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02c70b",
   "metadata": {},
   "source": [
    "2. Train the model $M$ only for $T_2$ on $D_2^{train}$. Call the trained model $M_{D_2}$. Evaluate $M_{D_2}$ on $D_2^{val}$ and report its performance metric (Accuracy) for the first task, ${Perf}_{T_2}(M_{D_2}|D_2^{val})$. Also print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc502c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5, Loss:0.44708535075187683\n",
      "Epoch:2/5, Loss:0.35330304503440857\n",
      "Epoch:3/5, Loss:0.28106847405433655\n",
      "Epoch:4/5, Loss:0.2198081612586975\n",
      "Epoch:5/5, Loss:0.18798157572746277\n",
      "Save Model in Task_1/trained_model\n",
      "Task_1 F1 on val Dataset:0.80\n",
      "Task_1 confusion_matrix:\n",
      "[[766 108]\n",
      " [146 503]]\n"
     ]
    }
   ],
   "source": [
    "task_type, metric_type, epoch, lr, weight_decay = \"Task_1\", \"F1\", 5, 2e-5, 1e-4\n",
    "train(Task_1_Bert, Task_1_train_Dataloader, Task_1_val_Dataloader, task_type, metric_type, epoch, lr, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d8d5e",
   "metadata": {},
   "source": [
    "2. Train the model $M$ only for $T_2$ on $D_2^{train}$. Call the trained model $M_{D_2}$. Evaluate $M_{D_2}$ on $D_2^{val}$ and report its performance metric (Accuracy) for the first task, ${Perf}_{T_2}(M_{D_2}|D_2^{val})$. Also print the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e16e92f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5, Loss:0.5994905829429626\n",
      "Epoch:2/5, Loss:0.4645318388938904\n",
      "Epoch:3/5, Loss:0.37755507230758667\n",
      "Epoch:4/5, Loss:0.3003614544868469\n",
      "Epoch:5/5, Loss:0.2376321703195572\n",
      "Save Model in Task_2/trained_model\n",
      "Task_2 Acc on val Dataset:0.78\n",
      "Task_2 confusion_matrix:\n",
      "[[1182  324   56]\n",
      " [ 232 1680  318]\n",
      " [  34  254 1417]]\n"
     ]
    }
   ],
   "source": [
    "task_type, metric_type, epoch, lr, weight_decay  = \"Task_2\", \"ACC\", 5, 2e-5, 1e-4\n",
    "train(Task_2_Bert, Task_2_train_Dataloader, Task_2_val_Dataloader, task_type, metric_type, epoch, lr, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce256c",
   "metadata": {},
   "source": [
    "3. Estimate the sentiment labels for the tweets in $D_1$: For all text in $D_1$, create $\\^{label^2}$ from the ${pred}^2$ output of $M_{D_2}(text)$. This will give you an augmented dataset $\\^{D_1} = {({text}_i, {label}_i^1, \\^{label}^2_1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86342c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loadding Model's Paramater which Trained on D1!\n",
      "Finish the Predcition on Task_1 by Task_2_Bert!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loadding Model's Paramater which Trained on D1!\")\n",
    "trained_model_path = \"Task_2/trained_model/pytorch_model.bin\"\n",
    "Task_2_Bert.load_state_dict(torch.load(trained_model_path))\n",
    "def Prediction(model, Dataloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    prediction_label_list = []\n",
    "    for index, batch in enumerate(Dataloader):\n",
    "        output = model(**batch)\n",
    "        prediction_label = output.logits.argmax(1)\n",
    "        prediction_label_list.extend(prediction_label.cpu().numpy().tolist())\n",
    "    print(\"Finish the Predcition on Task_1 by Task_2_Bert!\")\n",
    "    return prediction_label_list\n",
    "# import copy\n",
    "Task_1_train_X, Task_1_train_Y, pretrain_path = Task_1_train['text'], Task_1_train['target'], \"./roberta/roberta_base/roberta_base\"\n",
    "Task_1_train_all_Dataset = Task_Dataset(Task_1_train_X, Task_1_train_Y, pretrain_path)\n",
    "Task_1_train_all_Dataloader = get_dataloader(Task_1_train_all_Dataset, 8, False)\n",
    "Task_1_train_all_label_based_on_Task_2_Bert = Prediction(Task_2_Bert, Task_1_train_all_Dataloader)\n",
    "Task_1_Augmented_Dataset = copy.deepcopy(Task_1_train)\n",
    "Task_1_Augmented_Dataset['sentiment'] = Task_1_train_all_label_based_on_Task_2_Bert\n",
    "Task_1_Augmented_Dataset.to_csv(\"Task_1_Augmented_Dataset/data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e708fca7",
   "metadata": {},
   "source": [
    "4. Create another dataset by combining $D_1^{train}$ and $D_2^{train}$.\n",
    "\n",
    "Noted: this dataset has missing labels and is different from the augmented dataset $\\hat{D}_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dcbfd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Task_12_train = pd.DataFrame()\n",
    "Task_12_train['text'] = Task_1_train['text'].tolist() + Task_2_data['text'].tolist()\n",
    "Task_1_train_label = Task_1_train['target'].tolist() + [None] * len(Task_2_data)\n",
    "Task_2_train_label = [None] * len(Task_1_train) + Task_2_data['sentiment'].tolist()\n",
    "Task_12_train['target']  = Task_1_train_label\n",
    "Task_12_train['sentiment'] = Task_2_train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2bd2ae",
   "metadata": {},
   "source": [
    "5. We want to train $M$ for both $T_1$ and $T_2$ on $D_{12}$ by minimizing a weighted loss $\\lambda_1 l_1 + \\lambda_2 l_2$, where $\\lambda$ s are positive scaler weights between 0 and 1: higher the $\\lambda$ more the emphasis on corresponding task while training; $l_1$, $l_2$ are task-specific loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad949380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train model for both T1 and T2 by minimizing a weighted loss\n",
    "class Combined_T1_T2_Dataset(Dataset):\n",
    "    def __init__(self, dataset, pretrain_path):\n",
    "        super(Combined_T1_T2_Dataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(pretrain_path)\n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset['text'].iloc[index]\n",
    "        if not pd.isnull(self.dataset['target'].iloc[index]):\n",
    "            target = self.dataset['target'].iloc[index]\n",
    "        else:\n",
    "            target = -100\n",
    "        if not pd.isnull(self.dataset['sentiment'].iloc[index]):\n",
    "            sentiment = self.dataset['sentiment'].iloc[index]\n",
    "        else:\n",
    "            sentiment = -100\n",
    "        encode_dict = self.tokenizer.encode_plus(text=text, max_length=512,padding='max_length', truncation=True)\n",
    "        return encode_dict['input_ids'], encode_dict['attention_mask'], target, sentiment\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "Task_12_train_dataset = Combined_T1_T2_Dataset(Task_12_train, pretrain_path)\n",
    "Task_1_Augmented_df = pd.read_csv('Task_1_Augmented_Dataset/data.csv')\n",
    "Task_1_Augmented_val_df = Task_1_Augmented_df[int(len(Task_1_Augmented_df)*0.8):]\n",
    "Task_1_Augmented_val_dataset = Combined_T1_T2_Dataset(Task_1_Augmented_val_df, pretrain_path)\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    def padding_collate(batch):\n",
    "        (input_ids, attention_mask, targets, sentiments) = zip(*batch)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "        attention_mask = torch.tensor(attention_mask).to(device)\n",
    "        targets = torch.tensor(targets).to(device)\n",
    "        sentiments = torch.tensor(sentiments).to(device)\n",
    "        return {\"input_ids\":input_ids, \"attention_mask\":attention_mask, \"targets\":targets, \"sentiments\":sentiments}\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=padding_collate)\n",
    "Task_12_train_dataloader = get_dataloader(Task_12_train_dataset, 8, False)\n",
    "Task_1_Augmented_val_dataloader = get_dataloader(Task_1_Augmented_val_dataset, 8, False)\n",
    "class Task_12_model(nn.Module):\n",
    "    def __init__(self, pretrain_path, hidden_dim=768, task_1_label_num=2, task_2_label_num=3):\n",
    "        super(Task_12_model, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(pretrain_path)\n",
    "        self.disaster_classification = nn.Linear(hidden_dim, task_1_label_num)\n",
    "        self.sentiment_classification = nn.Linear(hidden_dim, task_2_label_num)\n",
    "    def forward(self, batch):\n",
    "        out = self.bert(input_ids=batch[\"input_ids\"], attention_mask= batch[\"attention_mask\"])\n",
    "        disaster_pred = self.disaster_classification(out[\"pooler_output\"])\n",
    "        sentiment_pred = self.sentiment_classification(out[\"pooler_output\"])\n",
    "        return disaster_pred, sentiment_pred\n",
    "def train(train_Dataloader, val_Dataloader, pretrain_path, save_path, lambdas_1=0.5, lambdas_2=0.5,epoch=1):\n",
    "    model = Task_12_model(pretrain_path)\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)  # AdamW优化器\n",
    "    crit_1, crit_2 = nn.CrossEntropyLoss(), nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        loss_all = 0\n",
    "        for index, batch in enumerate(train_Dataloader):\n",
    "            Task_1_prediction, Task_2_prediction = model(batch)\n",
    "            Task_1_true_label = batch[\"targets\"].long()\n",
    "            Task_2_true_label = batch[\"sentiments\"].long()\n",
    "            Task_1_loss = lambdas_1 * crit_1(Task_1_prediction, Task_1_true_label)\n",
    "            Task_2_loss = lambdas_2 * crit_2(Task_2_prediction, Task_2_true_label)\n",
    "            loss = Task_1_loss + Task_2_loss\n",
    "            loss_all += loss.data\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"Epoch:{}/{}, Loss:{}\".format(e + 1, epoch, loss_all / len(train_Dataloader)))\n",
    "    model.eval()\n",
    "    prediciton_label_Task_1_list = []\n",
    "    true_label_Task_1_list = []\n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(val_Dataloader):\n",
    "            Task_1_prob, Task_2_prob = model(batch)\n",
    "            prediciton_label_Task_1_list.extend(torch.argmax(Task_1_prob, dim=1).cpu().numpy().tolist())\n",
    "            true_label_Task_1_list.extend(batch[\"targets\"].cpu().numpy().tolist())\n",
    "    print(\"F1 on Task_1_val_Dataset:{:.2f}\".format(f1_score(true_label_Task_1_list, prediciton_label_Task_1_list)))\n",
    "    print(\"confusion_matrix:\")\n",
    "    print(confusion_matrix(true_label_Task_1_list, prediciton_label_Task_1_list))\n",
    "    print(\"Save Model!\")\n",
    "    torch.save(model.state_dict(), os.path.join(save_path, \"pytorch_model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd643d2",
   "metadata": {},
   "source": [
    "6. Disaster classification is our primary task. Obtain the best values of the hyper-parameters $\\lambda_1$, $\\lambda_2$ using any hyperparameter tuning method to optimise the metric ${Perf}_{T_1}(M_{D_{12}}|D_1^{val})$ for task 1. Note, $\\hat{D}_1^{val}$ and ${D}_1^{val}$ have the same labels for task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebee9e38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5, Loss:0.19117502868175507\n",
      "Epoch:2/5, Loss:0.15167516469955444\n",
      "Epoch:3/5, Loss:0.12343809008598328\n",
      "Epoch:4/5, Loss:0.10026174783706665\n",
      "Epoch:5/5, Loss:0.08251768350601196\n",
      "F1 on Task_1_val_Dataset:0.92\n",
      "confusion_matrix:\n",
      "[[756  58]\n",
      " [ 55 654]]\n",
      "Save Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5, Loss:0.3175782561302185\n",
      "Epoch:2/5, Loss:0.2452906221151352\n",
      "Epoch:3/5, Loss:0.20399875938892365\n",
      "Epoch:4/5, Loss:0.17204737663269043\n",
      "Epoch:5/5, Loss:0.14348483085632324\n",
      "F1 on Task_1_val_Dataset:0.76\n",
      "confusion_matrix:\n",
      "[[575 239]\n",
      " [124 585]]\n",
      "Save Model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/5, Loss:0.37667176127433777\n",
      "Epoch:2/5, Loss:0.2971390187740326\n",
      "Epoch:3/5, Loss:0.2372390180826187\n",
      "Epoch:4/5, Loss:0.19906166195869446\n",
      "Epoch:5/5, Loss:0.16163714230060577\n",
      "F1 on Task_1_val_Dataset:0.90\n",
      "confusion_matrix:\n",
      "[[787  27]\n",
      " [102 607]]\n",
      "Save Model!\n"
     ]
    }
   ],
   "source": [
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "save_path = \"./Task_12/trained_model\"\n",
    "train(Task_12_train_dataloader, Task_1_Augmented_val_dataloader, pretrain_path, save_path, 0.5, 0.3, 5)\n",
    "train(Task_12_train_dataloader, Task_1_Augmented_val_dataloader, pretrain_path, save_path, 0.5, 0.5, 5)\n",
    "train(Task_12_train_dataloader, Task_1_Augmented_val_dataloader, pretrain_path, save_path, 0.5, 0.7, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657cbcbd",
   "metadata": {},
   "source": [
    "7. Train the model $M$ on $D_{12}^{train}$ with the best $\\lambda$ s obtained in step 6. Call it $M_{{D^{\\ast}}_{12}}$. Evaluate it on $D_1^{test}$ for the tasks and report ${Perf}_{T_1}(M^{\\ast}_{D_{12}}|D_1^{test})$ by participating in the challenge. Submit a screenshot of your leaderboard score and position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab363a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Paramater of the model M on T_12_train!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./roberta/roberta_base/roberta_base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading!\n",
      "Finish Prediction!\n"
     ]
    }
   ],
   "source": [
    "# eval Task_1_test by using the model M on T_12_train with the best lambdas obtained in pre-step.\n",
    "print(\"Loading Paramater of the model M on T_12_train!\")\n",
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "model = Task_12_model(pretrain_path)\n",
    "trained_model_path = \"./Task_12/trained_model\"\n",
    "model.load_state_dict(torch.load(os.path.join(trained_model_path,\"pytorch_model.pth\")))\n",
    "def get_dataloader(dataset, batch_size, shuffle):\n",
    "    def pad_collate(batch):\n",
    "        (input_ids, attention_mask) = zip(*batch)\n",
    "        input_ids = torch.tensor(input_ids).to(device)\n",
    "        attention_mask = torch.tensor(attention_mask).to(device)\n",
    "        return {\"input_ids\":input_ids, \"attention_mask\":attention_mask}\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=pad_collate)\n",
    "print(\"Finish Loading!\")\n",
    "Task_1_test_dataset = Text_Dataset(Task_1_test, pretrain_path)\n",
    "Task_1_test_dataloader = get_dataloader(Task_1_test_dataset, 8, False)\n",
    "def test(model, Dataloader):\n",
    "    res = pd.DataFrame()\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    prediciton_label_Task_1_list = []\n",
    "    for index, batch in enumerate(Dataloader):\n",
    "        Task_1_prob, Task_2_prob = model(batch)\n",
    "        prediciton_label_Task_1_list.extend(torch.argmax(Task_1_prob, dim=1).cpu().numpy().tolist())\n",
    "    res['prediciton'] = prediciton_label_Task_1_list\n",
    "    res.to_csv(\"./submit/test_predict.csv\")\n",
    "    print(\"Finish Prediction!\")\n",
    "test(model, Task_1_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2ca10",
   "metadata": {},
   "source": [
    "8. In step 7, you performed multi-task learning, where data for the second task is externally sourced. Is the best value of hyper-parameter $\\lambda_2$ from step 6 zero or postive, i.e. is $\\lambda_2 = 0$ or $\\lambda_2 > 0$? What does the value of $\\lambda_2$ convey? Does the externally sourced sentiment data improve the model accuracy for our primary task of disaster classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98227eb4",
   "metadata": {},
   "source": [
    "9. For multi-task learning, why is the augmented dataset $\\hat{D_1}$ used only for testing but not for training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f8e09",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed48860f",
   "metadata": {},
   "source": [
    "1. How can the MTL strategy of loss combination in question 5 (part III) be used with a Random Forest? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251da93",
   "metadata": {},
   "source": [
    "2. Try the stated approach. Evaluate the model on $\\hat{D}_1^{val}$ and report the performance for both the tasks. Print confusion matrics for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b944df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize train text and val text!\n",
      "train word2vec!\n",
      "Training random forest!\n",
      "F1 on Task_1_Augmented_train_data_Task_1 Using random forest:0.57\n",
      "confusion_matrix:\n",
      "[[532 282]\n",
      " [312 397]]\n"
     ]
    }
   ],
   "source": [
    "# Try the approach based on Random Forest\n",
    "Task_1_Augmented_df = pd.read_csv(\"./Task_1_Augmented_Dataset/data.csv\")\n",
    "Task_1_Augmented_train_data = Task_1_Augmented_df[:int(len(Task_1_Augmented_df)*0.8)]\n",
    "Task_1_Augmented_val_data = Task_1_Augmented_df[int(len(Task_1_Augmented_df)*0.8):]\n",
    "def sentence_tokenize(datasets, pretrain_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrain_path)\n",
    "    tokenize_sentences= []\n",
    "    for i in range(len(datasets)):\n",
    "        tokenize_sentences.append(tokenizer.tokenize(datasets['text'].iloc[i].strip()))\n",
    "    return tokenize_sentences\n",
    "pretrain_path = \"./roberta/roberta_base/roberta_base\"\n",
    "print(\"tokenize train text and val text!\")\n",
    "train_tokenize_sentences = sentence_tokenize(Task_1_Augmented_train_data, pretrain_path)\n",
    "val_tokenize_sentences = sentence_tokenize(Task_1_Augmented_val_data, pretrain_path)\n",
    "print(\"train word2vec!\")\n",
    "word2vec_model = word2vec.Word2Vec(train_tokenize_sentences + val_tokenize_sentences, \\\n",
    "                                   workers=4, \\\n",
    "                                   vector_size=300, min_count=40, \\\n",
    "                                   window=10, sample=1e-3)\n",
    "word2vec_model.init_sims(replace=True)\n",
    "index2word_set = set(word2vec_model.wv.index_to_key)\n",
    "def get_avg_feat(tokenize_sentences, nums_features, model, index2word_set):\n",
    "    sentences_embedding = []\n",
    "    for sentences in tokenize_sentences:\n",
    "        embedding = np.zeros((len(sentences), nums_features), dtype=\"float32\")\n",
    "        for index, word in enumerate(sentences):\n",
    "            if word in index2word_set:\n",
    "                embedding[index] = model.wv[word]\n",
    "        if len(sentences)!=0:\n",
    "            sentence_embedding = np.sum(embedding, axis=0) / len(sentences)\n",
    "        \n",
    "        sentences_embedding.append(sentence_embedding)\n",
    "    return sentences_embedding\n",
    "trainVec = get_avg_feat(train_tokenize_sentences, 300, word2vec_model, index2word_set)\n",
    "valVec = get_avg_feat(val_tokenize_sentences, 300, word2vec_model, index2word_set)\n",
    "forest = RandomForestClassifier(n_estimators=1)\n",
    "print(\"Training random forest!\")\n",
    "def get_sample_weight(df):\n",
    "    sample_weight = {0 : 0.5, 1 : 0.5, 2 : 0.5}\n",
    "    return sample_weight[df['sentiment']]\n",
    "Task_1_Augmented_train_data['sample_weight'] = Task_1_Augmented_train_data.apply(get_sample_weight, axis=1)\n",
    "forest = forest.fit(trainVec, Task_1_Augmented_train_data[\"target\"], sample_weight=Task_1_Augmented_train_data[\"sample_weight\"])\n",
    "Task_1_val_prediction = forest.predict(valVec)\n",
    "print(\"F1 on Task_1_Augmented_train_data_Task_1 Using random forest:{:.2f}\".format(f1_score(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction)))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36d437",
   "metadata": {},
   "source": [
    "3. Use any hyper parameter tuning method to find the best values of the $\\lambda$ s to maiximize ${Perf}_{T_1}(M_{\\hat{D}_1^{train}}|D_1^{val})$. Is the optimal value result in $\\lambda_p = \\lambda_n = \\lambda_0$? If not, what is the ordering of $\\lambda$ s, and what can we infer from this ordering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9ef342",
   "metadata": {},
   "source": [
    "4. Train the model with the best $\\lambda$ s obtained in step 3. Evaluate it on $\\hat{D}_1^{test}$ for the tasks and report ${Perf}_{T_1}(M_{\\hat{D}_1^{train}}|D_1^{test})$ by participating in the challenge. Submit a screenshot of your leaderboard score and position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba92391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Finish!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it on Task_1_test_data\n",
    "# load datasets\n",
    "def load_datasets(Task_1_folder, Task_2_folder):\n",
    "    # read csv file\n",
    "    Task_1_train_file_path = os.path.join(Task_1_folder, \"data/train.csv\")\n",
    "    Task_1_test_file_path = os.path.join(Task_1_folder, \"data/test.csv\")\n",
    "    Task_2_data_file_path = os.path.join(Task_2_folder, \"data/Tweets.csv\")\n",
    "    Task_1_train = pd.read_csv(Task_1_train_file_path)\n",
    "    Task_1_test = pd.read_csv(Task_1_test_file_path)\n",
    "    Task_2_data = pd.read_csv(Task_2_data_file_path)\n",
    "    # nan text processing and label mapping\n",
    "    Task_2_data['text'] = Task_2_data['text'].replace(np.nan,\" \")\n",
    "    label2index = {\n",
    "        \"negative\":0,\n",
    "        \"neutral\":1,\n",
    "        \"positive\":2\n",
    "    }\n",
    "    Task_2_data['sentiment'] = Task_2_data['sentiment'].map(label2index)\n",
    "    return Task_1_train, Task_1_test, Task_2_data\n",
    "Task_1_folder = \"./Task_1\"\n",
    "Task_2_folder = \"./Task_2\"\n",
    "Task_1_train, Task_1_test, Task_2_data = load_datasets(Task_1_folder, Task_2_folder)\n",
    "\n",
    "test_tokenize_sentences = sentence_tokenize(Task_1_test, pretrain_path)\n",
    "testVec = get_avg_feat(test_tokenize_sentences, 300, word2vec_model, index2word_set)\n",
    "Task_1_prediction = forest.predict(testVec)\n",
    "res = pd.DataFrame()\n",
    "res['prediciton'] = Task_1_prediction\n",
    "res.to_csv(\"./submit/test_predict.csv\")\n",
    "print(\"Predict Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090ac29",
   "metadata": {},
   "source": [
    "## Further discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d555b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 on Task_1_Augmented_train_data_Task_1 using SVM:0.67\n",
      "confusion_matrix:\n",
      "[[579 235]\n",
      " [229 480]]\n"
     ]
    }
   ],
   "source": [
    "# approach_1 SVM\n",
    "svc = SVC(kernel = 'linear')\n",
    "svc.fit(trainVec, Task_1_Augmented_train_data[\"target\"], sample_weight=Task_1_Augmented_train_data[\"sample_weight\"])\n",
    "Task_1_val_prediction = svc.predict(valVec)\n",
    "print(\"F1 on Task_1_Augmented_train_data_Task_1 using SVM:{:.2f}\".format(f1_score(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction)))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a30723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 on Task_1_Augmented_train_data_Task_1 using GBDT:0.64\n",
      "confusion_matrix:\n",
      "[[634 180]\n",
      " [292 417]]\n"
     ]
    }
   ],
   "source": [
    "# approach_2 gbdt\n",
    "GBDT = GradientBoostingClassifier()  # adaboost\n",
    "GBDT = GBDT.fit(trainVec, Task_1_Augmented_train_data[\"target\"], sample_weight=Task_1_Augmented_train_data[\"sample_weight\"])  # 拟合训练集\n",
    "Task_1_val_prediction = GBDT.predict(valVec)\n",
    "print(\"F1 on Task_1_Augmented_train_data_Task_1 using GBDT:{:.2f}\".format(f1_score(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction)))\n",
    "print(\"confusion_matrix:\")\n",
    "print(confusion_matrix(Task_1_Augmented_val_data[\"target\"], Task_1_val_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
